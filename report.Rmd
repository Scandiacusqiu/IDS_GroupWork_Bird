---
title: "Modelling Bird Characteristics"
author: "by Pipeline: Naomi Pihlaja, Jiaxi Yan, Yupeng Qiu, Jiayi Chen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load-lib, include = FALSE}
library(tidyverse)
library(tidymodels)



```


```{r load-data, include=FALSE}
data <- read_csv("data/AVONET3_BirdTree.csv")

```


## Research Question

**Can we accurately predict certain characteristics of birds using chosen explanatory variables?** 

\n
(If we can, we could potentially predict the lifestyle elements of lesser known/ lesser investigated bird species: these predictions could be used in scientific research for wildlife conservation and investigation.)


## Data

figshare.com. (n.d.). AVONET: morphological, ecological and geographical data for all birds (Tobias et al 2022 Ecology Letters doi: https://doi.org/10.111/ele.13898). [online] Available at: https://figshare.com/s/b990722d72a26b5bfead.









# Introduction

A bird's ecological traits, especially morphological, offer valuable insights into how they adapt to their environments, and thus also offer insights into the conditions of their environment. The AVONET Birdtree dataset includes data about thousands of bird species. Most of the data concerns measurements on their morphology (eg. Beak width), but it also contains data on their habitats and trophic ecology. This dataset allows us to investigate hypothesis regarding potential correlations of different bird traits, and of potential predictive modelling.

\n
Our investigation concerned two key questions:

\n
**1. To what extent can morphological traits explain ecological characteristic such as trophic level or latitude?**
**2. Which morphological variables show meaningful correlations with each other?**

\n
These questions were motivated by our hypothesis generated during group discussions, using logical thinking based on our knowledge of biology and animal adaptations based on lifestyle.

# Data Overview

Avonet birdtree contains summary statistics on specific characteristics of birds. Many are numerical variables (eg. Beak depth), but some are categorical (eg. Trophic level).Over 10,000 observations allows for a big sample size.

\n
Our variables of interest (data we thought would be worth investigating after looking through the dataset) were as follows:

\n
- **Habitat**: Where the bird lives

- **Trophic.level**: Categorical variable based on the composition of a birds' diet

- **Hangwingindex**: A measurement of a bird's wing shape that indicates flight efficiency and dispersal ability 

- **Beak.width**: Width of the beak at the anterior edge of the nostrils

- **Beak.depth**: Depth of the beak at the anterior edge of the nostrils

- **Centroid.Latitude**: The geometric centre of the species range

- **Mass** : Body mass of species average

- **Wing.length** : Length from the carpal joint to the tip of the longest primary on the unflattened wing



# Exploratory Statistics
### 1. Correlation: Beak Width & Beak Depth

\n
We began by exploring any correlations between traits we suspected to be related, based on our knowlegde of basic bird biology. After constructing a scatterplot with a linear model, the line of best fit indicated a strong positive correlation between beak width and beak depth.

```{r beak length and depth correlation, echo=FALSE}
#Hypothesis: Bird beak depth increases as beak length increases. Positive correlation.

ggplot(data, aes(x=Beak.Width, y=Beak.Depth, color = Centroid.Latitude))+
         geom_point()+
  geom_smooth(method= "lm")+
  labs(title = "Bird beak width & depth",
       x = "Beak width (mm)", 
       y = "Beak depth (mm)")

```

### 2.Predicting Latitude

\n
We then attempted to predict centroid latitude using these morphological variables (beak width and length). However, our model showed no meaningful visual correlation- the colours were mixed throughout the plane of the graph. This suggested that the latitude of a bird's habitat do not strongly affect their beak shape. The lack of correlation was confirmed by our model whcih attempted to use beak shape to predict latitude:
```{r predicting latitude, echo=FALSE}

# Start modelling
set.seed(2801)
data_split1 <- initial_split(data, 0.8)
data_train1 <- training(data_split1)
data_test1 <- testing(data_split1)


# Construct workflow
data_rec1 <- recipe(Centroid.Latitude ~ Beak.Width + Beak.Depth, data = data) %>% 
  step_naomit(all_predictors()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

data_mod1 <- linear_reg() %>% 
  set_engine("lm")

data_wflow1 <- workflow() %>% 
  add_model(data_mod1) %>% 
  add_recipe(data_rec1)

data_fit1 <- data_wflow1 %>% 
  fit(data = data_train1)
# tidy(data_fit)

data_pred1 <- predict(data_fit1, data_test1) %>% 
  bind_cols(data_test1) %>% 
  print(n = 10)

```

\n
The RMSE of **22.2** suggests an average error of 22.2 when predicting latitude. This is a very high error in the context of latitude, which ranges from -90 to 90 (around a 12% error). A very low R squared value of **0.002** suggests that the model does very little to explain the variance in latitude.
```{r predicting latitude split test, echo=FALSE}
# Check model performance
folds1 <- vfold_cv(data_train1, v = 5)
data_fit_rs1 <- data_wflow1 %>% 
  fit_resamples(folds1)
collect_metrics(data_fit_rs1)

```


### 3. Exploring Trophic Level

\n
Our further explorations investigated whether the geographic range size of a species could predict whether it belonged to the trophic category scavenger.

\n
First, we constructed a visualization of the range Size of birds at different trophic levels via a jitter plot. The graph showed that there are some differences in the distribution range of different trophic groups: the range size of carnivorous species is larger and more scattered, while herbivores and omnivores usually have a more medium-sized and more concentrated distribution range. The range of scavengers is relatively small. Overall, there was a weak correlation between trophic level and range size, but there was still some evidence of a distributive trend across species.

```{r exploring trophic level using geom_jitter,  echo=FALSE}

data <- data %>% 
  mutate(Trophic.Level = as.factor(Trophic.Level))

data_clean <- data %>%
    filter(!is.na(Range.Size),
           !is.na(Trophic.Level))

model_trophic_range <- lm(Range.Size ~ Trophic.Level, data = data_clean)
summary(model_trophic_range)

ggplot(data_clean, aes(x = Trophic.Level, y = Range.Size)) +
    geom_jitter()

```

\n
We then decided to try to use a model to predict whether or not a bird was a scavenger by using its range Size value. As we planned to use a logistic regression model, we re-encoded trophic Level as a binary categorical variable, and distinguish scavengers ("yes") from non-scavengers ("no").  


```{r exploring trophic level using modelling, echo=FALSE}
data$Scavenger <- factor(ifelse(data$Trophic.Level == "Scavenger", "yes", "no"),
                       levels = c("no", "yes"))

fit_scav <- logistic_reg() %>% 
  set_engine("glm") %>%
  fit(Scavenger ~ Range.Size, data = data)

data_pred <- data %>%
  mutate(.pred_yes = predict(fit_scav, data, type = "prob")$.pred_yes)

data_pred %>%
  roc_curve(Scavenger, .pred_yes, event_level = "second") %>%
  autoplot()
```

\n
Then, using range size as the only independent variable, we constructed a logistic regression model. The resulting AUC was **0.8027**, indicating that the model is somewhat successful in predicting whether a bird is a scavenger, and that there is are significant differences among species of different range sizes based on whether they are scavengers or not.

```{r exploring trophic level using modelling AUC value, echo=FALSE}
data_pred %>%
  roc_auc(Scavenger, .pred_yes, event_level = "second")

data_pred2 <- data_pred %>%
  mutate(Scavenger_num = if_else(Scavenger == "yes", 1, 0))
```

\n
However, a limitation of the model is the very small sample size of scavengers within the data, which was **20**.

```{r size of trophic levels, echo= FALSE}
data %>%
  count(Trophic.Level, name = "sample_size")

```


### 4. Predicting Trophic Niche
\n
As we were unsuccessful in predicting trophic level, we moved onto attempting to predict the variable trophic niche: more specifically, whether a bird is an invertivore. We selected hand wing index as a variable and fitted a logistic regression model:

```{r invertivore ~ handwingindex, echo=FALSE}

# Choose needed data
data_hwi <- data %>% 
  select(Trophic.Niche, `Hand-Wing.Index`) %>% 
  mutate(Invertivore = case_when(Trophic.Niche == "Invertivore" ~ "1", 
                       TRUE ~ "0"))
set.seed(8888)
data_split1 <- initial_split(data_hwi, 0.8)
data_train1 <- training(data_split1)
data_test1 <- testing(data_split1)

# Construct workflow
data_rec1 <- recipe(Invertivore ~ `Hand-Wing.Index`, data = data_hwi) %>% 
  step_naomit(all_predictors()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

data_mod1 <- logistic_reg() %>% 
  set_engine("glm")

data_wflow1 <- workflow() %>% 
  add_model(data_mod1) %>% 
  add_recipe(data_rec1)

data_fit1 <- data_wflow1 %>% 
  fit(data = data_train1)

# tidy(data_fit)
data_pred1 <- predict(data_fit1, data_test1, type = "prob") %>% 
  bind_cols(data_test1) %>% 
  print(n = 10)
```

\n
We then checked the model performance, which revealed a relatively high AUC of **0.73**, which suggests it's relatively useful in predicting whether the bird is invertivore or not.

```{r check model performence, echo=FALSE}
cut_off_prob <- 0.45
data_pred1 <- data_pred1 %>% 
  mutate(Invertivore_fac = factor(Invertivore), 
         Invertivore_num = as.numeric(Invertivore), 
         .pred = case_when(.pred_1 > cut_off_prob ~ 1, 
                           TRUE ~ 0))

data_pred1 %>% 
  mutate(
    response = if_else(Invertivore == "1", "Invertivore", "Not Invertivore"), 
    Invertivore_pred = if_else(.pred_1 >= cut_off_prob, "Predicted Invertivore", "Predicted Not Invertivore")
  )  %>% 
  count(response, Invertivore_pred) %>% 
  pivot_wider(names_from = Invertivore_pred, values_from = n)

data_pred1 %>%
  roc_curve(
    truth = Invertivore_fac,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()

data_pred1 %>%
  roc_auc(
    truth = Invertivore_fac,
    .pred_1,
    event_level = "second"
  )


folds1 <- vfold_cv(data_train1, v = 5)
data_fit_rs1 <- data_wflow1 %>% 
  fit_resamples(folds1)
collect_metrics(data_fit_rs1)

```

### 5. Improving our prediction on Trophic Niche
\n 
In order to improve the model’s ability to distinguish invertivores from other birds, we added a new variable, mass, to the model. We came to this decision after constructing a ```geom_jitter()``` plot, which visually suggested some correlation between trophic niche and mass:

```{r find mass could be a predictor as well, echo=FALSE}
data %>%
  ggplot(aes(x = log(Mass), y = Trophic.Niche)) +
  geom_jitter()
```

\n
We then fitted a model to our chosen variables:

```{r invertivore ~ handwingindex + Mass, echo=FALSE}
# Choose needed data
data_hwi2 <- data %>% 
  select(Trophic.Niche, `Hand-Wing.Index`, Mass) %>% 
  mutate(Invertivore = case_when(Trophic.Niche == "Invertivore" ~ "1", 
                       TRUE ~ "0"))
set.seed(8888)
data_split2 <- initial_split(data_hwi2, 0.8)
data_train2 <- training(data_split2)
data_test2 <- testing(data_split2)

# Construct workflow
data_rec2 <- recipe(Invertivore ~ `Hand-Wing.Index` + Mass, data = data_train2) %>% 
  step_naomit(all_predictors()) %>% 
  step_dummy(all_nominal(), -all_outcomes())

data_mod2 <- logistic_reg() %>% 
  set_engine("glm")

data_wflow2 <- workflow() %>% 
  add_model(data_mod2) %>% 
  add_recipe(data_rec2)

data_fit2 <- data_wflow2 %>% 
  fit(data = data_train2)
# tidy(data_fit)

data_pred2 <- predict(data_fit2, data_test2, type = "prob") %>% 
  bind_cols(data_test2) %>% 
  print(n = 10)
```

\n
We then tested our model's performance:

```{r check model performence for trophic niche, echo=FALSE}
cut_off_prob <- 0.45
data_pred2 <- data_pred2 %>% 
  mutate(Invertivore_fac = factor(Invertivore), 
         Invertivore_num = as.numeric(Invertivore), 
         .pred = case_when(.pred_1 > cut_off_prob ~ 1, 
                           TRUE ~ 0))

data_pred2 %>% 
  mutate(
    response = if_else(Invertivore == "1", "Invertivore", "Not Invertivore"), 
    Invertivore_pred = if_else(.pred_1 >= cut_off_prob, "Predicted Invertivore", "Predicted Not Invertivore")
  )  %>% 
  count(response, Invertivore_pred) %>% 
  pivot_wider(names_from = Invertivore_pred, values_from = n)

data_pred2 %>%
  roc_curve(
    truth = Invertivore_fac,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()

data_pred2 %>%
  roc_auc(
    truth = Invertivore_fac,
    .pred_1,
    event_level = "second"
  )


folds1 <- vfold_cv(data_train2, v = 5)
data_fit_rs2 <- data_wflow2 %>% 
  fit_resamples(folds1)
collect_metrics(data_fit_rs1)


TP<-1047
TN<-715
FP<- 586
FN<- 151

Sensitivity <- TP/(TP+FN) 
Sensitivity

Specificity <- TN/(TN+FP)
Specificity
```
\n
 The AUC increased from **0.73** to **0.79**, which suggests an improvement on the model's ability to predict whether a bird is an invertivore.
 
# 6. Generalisation Performance (Train–Test Split Evaluation)
\n 
To assess how well our models generalised beyond the training data, we applied a train–test split and evaluated the model on a separate test set that was not used during model fitting, to check whether the patterns learned in the training process holds up when the model encounters new observations:

\n
The test-set metrics therefore provide a more realistic indication of predictive performance. 

\n
First, we applied the train-test split to Hand Wing Index alone:

```{r Train–Test Split (HWI-only), echo=FALSE}
data_hwi <- data %>% 
  select(Trophic.Niche, `Hand-Wing.Index`) %>% 
  mutate(Invertivore = ifelse(Trophic.Niche == "Invertivore", 1, 0)) %>% 
  mutate(Invertivore = as.numeric(Invertivore))

set.seed(8888)
hwi_split <- initial_split(data_hwi, prop = 0.8, strata = Invertivore)
hwi_train <- training(hwi_split)
hwi_test  <- testing(hwi_split)

hwi_model <- glm(Invertivore ~ `Hand-Wing.Index`,
                 data = hwi_train,
                 family = binomial)

hwi_test_pred <- hwi_test %>%
  mutate(
    Invertivore = factor(Invertivore),
    .pred       = predict(hwi_model, hwi_test, type = "response"),
    .pred_class = factor(ifelse(.pred > 0.45, "1", "0"))
  )

hwi_auc        <- roc_auc(hwi_test_pred, truth = Invertivore, .pred, event_level = "second")
hwi_accuracy   <- accuracy(hwi_test_pred, truth = Invertivore, estimate = .pred_class)
hwi_conf_mat   <- conf_mat(hwi_test_pred, truth = Invertivore, estimate = .pred_class)
hwi_sensitivity <- sens(hwi_test_pred, truth = Invertivore, estimate = .pred_class)
hwi_specificity <- spec(hwi_test_pred, truth = Invertivore, estimate = .pred_class)

hwi_auc
```

```{r Train–Test Split 1(HWI-only), echo=FALSE}
hwi_accuracy
```

```{r Train–Test Split 2(HWI-only), echo=FALSE}
hwi_conf_mat
```

```{r Train–Test Split 3(HWI-only), echo=FALSE}
hwi_sensitivity
```

```{r Train–Test Split 4(HWI-only), echo=FALSE}
hwi_specificity
```

\n
We then applied the train-test split to the Hand Wing Index AND mass:

```{r Train–Test Split (HWI + Mass), echo=FALSE}
data_hwi2 <- data %>% 
  select(Trophic.Niche, `Hand-Wing.Index`, Mass) %>% 
  mutate(Invertivore = ifelse(Trophic.Niche == "Invertivore", 1, 0)) %>%
  mutate(Invertivore = as.numeric(Invertivore))

set.seed(8888)
hwi_mass_split <- initial_split(data_hwi2, prop = 0.8, strata = Invertivore)
hwi_mass_train <- training(hwi_mass_split)
hwi_mass_test  <- testing(hwi_mass_split)

hwi_mass_model <- glm(Invertivore ~ `Hand-Wing.Index` + Mass,
                      data = hwi_mass_train,
                      family = binomial)

hwi_mass_test_pred <- hwi_mass_test %>%
  mutate(
    Invertivore = factor(Invertivore),
    .pred       = predict(hwi_mass_model, hwi_mass_test, type = "response"),
    .pred_class = factor(ifelse(.pred > 0.45, "1", "0"))
  )

hwi_mass_auc        <- roc_auc(hwi_mass_test_pred, truth = Invertivore, .pred, event_level = "second")
hwi_mass_accuracy   <- accuracy(hwi_mass_test_pred, truth = Invertivore, estimate = .pred_class)
hwi_mass_conf_mat   <- conf_mat(hwi_mass_test_pred, truth = Invertivore, estimate = .pred_class)
hwi_mass_sensitivity <- sens(hwi_mass_test_pred, truth = Invertivore, estimate = .pred_class)
hwi_mass_specificity <- spec(hwi_mass_test_pred, truth = Invertivore, estimate = .pred_class)

hwi_mass_auc
hwi_mass_accuracy
hwi_mass_conf_mat
hwi_mass_sensitivity
hwi_mass_specificity

```

\n
The training-set AUC value wss noticeably higher than the test-set AUC values: the sharp drop in AUC indicates that the models do not generalise well and may be overfit to the training data, which means its predictive ability is limited when applied to unseen cases.


# Discussions
\n 

### Successes

\n 
One advantage of our investigation was that it applied a range of statistical tools (eg. many visualisation methods, different statistical models), rather than relying on a single method. By using exploratory data analysis, several forms of regression, and multiple performance metrics, we were able to evaluate the dataset from different perspectives to understand both its structure and its limitations. 

\n
The project also allowed us to practise skills such as transforming variables, handling categorical outcomes, comparing models, and interpreting values like AUC, sensitivity, specificity, and confusion matrices within the context of our data. Alsp, our training and test AUC values were very similar, suggesting that our evaluation was reliable and not overly dependent on the training data. 

\n
Overall, our analysis demonstrated our ability to think critically about modelling choices, assess whether a method is appropriate, and adjust our approach when our initial attempts did not produce meaningful results.

\n 

### Limitations

\n 
Our investigation, however, has several limitations. The early linear regressions showed that morphology had almost no ability to explain variables such as latitude, trophic level, or range size. Some categories, such as scavengers, had too few observations to support meaningful analysis. In the logistic phase, class imbalance made it difficult for the model to identify invertivores, and although the test-set AUC was similar to the training AUC, the overall predictive power remained moderate, meaning the model was only partially successful at separating invertivores from other trophic niches. The modelling techniques we used were also fairly simple, and our evaluation relied on a single train–test split, meaning the results may have varied depending on the way the data was divided. 

\n
Another limitation could be that each species within the data has multiple invidivuals, yet each species represents one value for each variable, the value of which is an average. If each individual bird held its own value for each of the variables, perhaps the correlations between and the predictive power of variables could have been more significant.

\n 

### Future Work

\n 
For improvement, we could expand the modelling approach by incorporating regression methods that offer more flexibility than the basic ones used in this investigation. This would allow us to capture patterns that simple linear or logistic models could not identify. We could also strengthen our evaluation process by applying cross-validation in logistic models, which would provide a more reliable assessment of the models' performance rather than relying on a single train–test split. By considering approaches to handling class imbalance and explore how different classification thresholds affect performance, we would also be able to improve our analysis to be more robust and better suited for complex datasets.

# Conclusion
\n 
Overall, our investigation showed that the ecological variables we initially explored (latitude, trophic level, and range size) had almost no relationship with the morphological traits in our dataset, as reflected by near-zero R² values and error in the linear models. When we shifted to a logistic regression framework to predict whether a bird is an invertivore, the training-set results improved after adding Mass as a predictor, suggesting that it contributed additional predictive power beyond Hand-Wing Index.The train and test AUC values (approximately 0.73 and 0.75, respectively) were very similar, indicating that the model generalised reasonably well rather than overfitting, although its predictive performance remained moderate. These findings demonstrated that overall predictive ability of the variables is limited, and more informative variables or a richer dataset would be required for stronger modelling outcomes.